---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

# title: "Real Exchange Rate Behaviour: A Replication and Robustness Check
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: TRUE
Entry1: "Real Exchange Rate Behaviour: A Replication and Robustness Check"
Entry2: "Cassandra Pengelly | 20346212" # textbf for bold
Entry3: "Econometrics 871: Time Series Project"
Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
Logo_width: 0.5
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
# AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
# Author1: "Nico Katzke^[__Contributions:__  \\newline _The authors would like to thank no institution for money donated to this project. Thank you sincerely._]"  # First Author - note the thanks message displayed as an italic footnote of first page.
# Ref1: "Prescient Securities, Cape Town, South Africa" # First Author's Affiliation
# Email1: "nfkatzke\\@gmail.com" # First Author's Email address
# 
# Author2: "John Smith"
# Ref2: "Some other Institution, Cape Town, South Africa"
# Email2: "John\\@gmail.com"
# CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.
# 
# Author3: "John Doe"
# Email3: "Joe\\@gmail.com"
# 
# CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE
# 
# # Comment out below to remove both. JEL Codes only given if keywords also given.
# keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
# JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: TRUE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
# abstract: |
#   Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
# Loading packages used
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("plm")) install.packages("plm")
library(bootUR)
library(tidyverse)
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))

ind <- read_table("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Metrics/Time Series Project/R stuffs/Write_Up/data/files/ind.dat")
lnk <- read_table("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Metrics/Time Series Project/R stuffs/Write_Up/data/files/lnk.dat")
mal <- read_table("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Metrics/Time Series Project/R stuffs/Write_Up/data/files/mal.dat")
mnr <- read_table("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Metrics/Time Series Project/R stuffs/Write_Up/data/files/mnr.dat")
pak <- read_table("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Metrics/Time Series Project/R stuffs/Write_Up/data/files/pak.dat")
phl <- read_table("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Metrics/Time Series Project/R stuffs/Write_Up/data/files/phl.dat")
thi <- read_table("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Metrics/Time Series Project/R stuffs/Write_Up/data/files/thi.dat")

# adding in a column to each data set for the real exchange rate

rexIND <- ind %>% mutate(ENTRY = as.factor(ENTRY)) %>% mutate(rexIND = real_exchange(INDEX, INDCPI, USCPI)) %>% select(ENTRY, rexIND)
rexLNK <- lnk %>% mutate(ENTRY = as.factor(ENTRY)) %>% mutate(rexLNK = real_exchange(LNKEX, LNKCPI, USCPI)) %>% select(rexLNK)
rexMAL <- mal %>% mutate(ENTRY = as.factor(ENTRY)) %>% mutate(rexMAL = real_exchange(MALEX, MALCPI, USCPI)) %>% select(rexMAL)
rexMNR <- mnr %>% mutate(ENTRY = as.factor(ENTRY)) %>% mutate(MNREX = replace(MNREX, 203, 16.5)) %>% mutate(rexMNR = real_exchange(MNREX, MNRCPI, USCPI)) %>% select(rexMNR) #replaced the value for the exchange rate for 1974:11 with 16.5 (was 1.45)
rexPAK <- pak %>% mutate(ENTRY = as.factor(ENTRY)) %>% mutate(rexPAK = real_exchange(PAKEX, PAKCPI, USCPI)) %>% select(rexPAK)
rexPHL <- phl %>% mutate(ENTRY = as.factor(ENTRY)) %>% mutate(PHLEX = replace(PHLEX, 213, 7.7)) %>% mutate(rexPHL = real_exchange(PHLEX, PHLCPI, USCPI)) %>% select(rexPHL) #replaced the value for the exchange rate for 1975:09 with 7.7 (was 0.7)
rexTHI <- thi %>% mutate(ENTRY = as.factor(ENTRY)) %>% mutate(rexTHI = real_exchange(THIEX, THICPI, USCPI)) %>% select(rexTHI)

# binding the columns together for a panel data set of real exchange rates

pdata <- bind_cols(rexIND,rexLNK, rexMAL, rexMNR,  rexPAK,  rexPHL,  rexTHI)
```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->
\newpage

# Introduction \label{Introduction}

How do we compare living standards and economic productivity between countries? This is one of the questions that macroeconomics attempts to answers and a number of tools have been developed within the field to this end. One of these tools is the Purchasing Power Parity (PPP) theory, which uses a basket of goods to compare the currencies of different countries. This theory has been widely tested using data, and the results have been divisive and somewhat puzzling (@puz).

In this essay, I replicate^[More accurately, try my best to replicate] the paper "Real Exchange Rate Behaviour: Evidence from Black Markets" by @Kul, which tests the PPP hypothesis. @Kul finds that the behaviour of the real exchange rate is mean-reverting in the long-run, which suggests that the PPP theory is empirically supported. I include some other tests in addition to those presented in the paper as a robustness check on these results. 

This essay^[This essay was written in R using the package by @Texevier] is organised as follows. Section \ref{Context} contextualises Luintel's paper and discusses the robustness checks. Section \ref{Data} discusses the data and reports the results of the Wald-Wolfowitz tests. Section \ref{Unit} deals with the unit root tests. Section \ref{Var} reports the results of the variance ratio test.The code for this replication can be found on Github [here](https://github.com/cass-code/metrics.git).


# Context and Evaluation \label{Context}

@Kul investigates whether the PPP hypothesis holds empirically. To test this theory, @Kul uses monthly black market real exchange rates (in terms of the US dollar) from eight developing Asian countries: India, Sri Lanka, Myanmar, Malaysia, Pakistan, Philippines, Taiwan and Thailand. Using data from developing countries (rather than from developed countries) was a novel approach for its time. The black market rates are used as a proxy for the float rates of developing countries. 

Practically, the paper has two main aims: the first is to determine whether there are segmented trends in the data, and the second is to test whether the panel data is stationary. At the time that this paper was written (early 2000s), the puzzle of PPP was that tests for unit roots failed to reject the null hypothesis. The null hypothesis in these cases was the presence of unit roots; these tests implied non-stationarity and discredited PPP, despite the support from economic theory(@puz). 

@Kul makes use of (more) powerful unit root tests for heterogeneous panels, and finds that real exchange rates are mean-reverting. This was novel for the time as most time-series studies rejected PPP and concluded that the real exchange rate followed a random walk. This suggested that any shocks to the real exchange rate were persistent and there was no mean-reversion either in the short or long term (@rog). @Kul finds that the black market real exchange rates do not behave in an excessively volatile manner, which conflicted with the findings of the literature at that time. Additionally, the findings of the study implied that such empirical investigation may not necessarily suffer from survivorship bias.

A critical part of Luintel's paper is testing for unit roots in the panel data, specifically the paper makes use of the Im-Pesaran-Shin T-bar test. In addition to replicating this test, I implement several other unit root tests as a robustness check and find that the results are mixed. @Kul [p.70] defends the choice of the IPS tests well, citing that they allow for the dynamics and error variances across groups, and the T-bar tests based on meaned regressions are valid. Additionally, these tests may have better small sample propoerties. I run the IPS tests using @Kul's specified lags, and the AIC method. I then implement the panel stationarity tests proposed by @lev, @wu, @had, as well as a bootstrapped panel unit root test from @pal.   

# Data \label{Data}

The data used for the analysis is a series on black market nominal exchange rates and consumer price indices (CPI) for 8 developing Asian countries, namely: India, Sri Lanka, Myanmar, Malaysia, Pakistan, Philippines, Taiwan and Thailand. I take a subset of these countries by excluding Taiwan^[I excluded Taiwan because there is some data missing from the set and I don't know how to adjust an unbalanced panel. However, it is also interesting to test if the results hold when taking a subset] from the analysis. @Kul sources data from various issues of *Pick's Currency Year Book* and *World Currency Year Book*. The data used for Luintel's paper is accessible through the Journal of Applied Econometrics archive, which is where I attained my data. The sample period runs for 31 periods from January 1958 to June 1989. This sample period is split into two parts: Bretton Woods and after Bretton Woods (also referred to as pre-float period and the float period).

The nominal exchange rates are units currencies per unit of US dollar. There were two mistakes in the nominal exchange rate datasets: for Myanmar November 1974,  there was a value of 1.45, which I replaced with 16.5 (based on interpolation). And for the Philippines in September 1975,  there was a value of 0.7 with which I replaced with 7.7 (based on interpolation).^[I discovered these mistakes when there was a dramatic difference in my plots of the real exchange rates and Luintel's plots.] Luintel sources the CPI figures from issues of International Financial Statistics (which are included in Luintel's dataset available in the JAE data archives).  

To calculate the real exchange rates, I follow the lead of @Kul [p. 165] and apply the following formula to the nominal exchange rates:

$$
rex = log(Nominal Exchange Rate) - log(CPI) + log(United States CPI)
$$

I plot the real exchange rate series below in \ref{Figure1}. The plots below match those of @Kul [p. 166] and indicate that the real exchange rates are trending. Additionally, the graphs show that the black market exchange rates are somewhat volatile. As expected, we see that after the first oil shock of 1973 the currencies appreciated and then slowly reverted. The plots suggest that the trends are segmented. @Kul [p. 169] tests this hypothesis using formal tests, and I follow suit - the results of the Wald-Wolfowitz Tests are reported below the plots in \ref{wald}.

```{r Figure1, echo = FALSE, include= TRUE, warning =  FALSE, fig.height= 9, fig.align = 'center', fig.ext = 'png'}

library(tidyverse)
g <- rex_plots_combined(pdata)

```

```{r Figure2, echo = FALSE, include= TRUE, warning =  FALSE, fig.height= 9, fig.align = 'center', fig.cap = "Plot of Real Exchange Rates over Time\\label{Figure1}", fig.ext = 'png'}

library(tidyverse)
h <- rex_plots_combined1(pdata)

# had an issue with some extra grob table information printing - resolved this by including grid.draw in the plot function and not including "h" here in the chunk
```

## Wald-Wolfowitz Tests \label{wald}

```{r Figure3, echo = FALSE, include= TRUE, warning =  FALSE, fig.height= 5, fig.align = 'center', fig.ext = 'png', results='asis'}

# The Wald-Wolfowitz tests

library(xtable)
data <- wald(pdata) %>% tibble::as_tibble()

table <- xtable(data, caption = "Wald-Wolfowitz tests \\label{tab1}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             include.rownames = FALSE,
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top'
             )

```

# Unit Root Tests \label{Unit}

First, I employed the Augmented Dickey-Fuller test for the individual exchange rates to see whether there was a unit root present. The test results show that the individual time series are all non-stationary.


```{r ADF, echo = FALSE, include= TRUE, warning =  FALSE, results='asis'}
library(tidyverse)
library(xtable)
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))

bret <- pdata %>% slice_head(n =183) #prefloat era bretton woods
postbret <- pdata %>% slice_tail(n = 195) # float era post bretton woods
B1 <- (ADF(bret)) %>% select(-Countries) %>% rename("Bretton Woods (1958:1-1973:3)" = "Dickey-Fuller")
B2<- (ADF(postbret)) %>% select(-Countries) %>% rename("Post-Bretton Woods (1973:4-1989:6)" = "Dickey-Fuller")
A <- ADF(pdata) %>% rename("Full Sample" = "Dickey-Fuller")


list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
fullADF <- bind_cols(A, B1, B2)
data = fullADF %>% tibble::as_tibble()
  addtorow          <- list()
  addtorow$pos      <- list()
  addtorow$pos[[1]] <- c(0)
  addtorow$command  <- c(paste("\\hline \n",
                               "\\endhead \n",
                               "\\hline \n",
                               "{\\footnotesize Continued on next page} \n",
                               "\\endfoot \n",
                               "\\endlastfoot \n",sep=""))
table <- xtable(data, caption = "Augmented Dickey-Fuller Tests")
  print.xtable(table,
             tabular.environment = "longtable",
             floating = FALSE, # Leave this as is.
             table.placement = 'H', # Leave this as is.
             booktabs = T, # Aesthetics
             include.rownames = FALSE,  # Typically you don't want this in a table.
             add.to.row = addtorow, # For adding the Continued on next page part...
             comment = FALSE,
             caption.placement = 'top',  # Where do you want the caption?
             size="\\fontsize{11pt}{12pt}\\selectfont"  # Size of text in table..
             )
```

## Panel Unit Root Tests

As noted by @pes [p.18], when using country data for macroeconomic applications, there are often contemporaneous correlations within the time series, which is a relevant concern for testing the PPP hypothesis. There may be unobserved common factors or spatial spillover effects, which need to be accounted for in the unit root test. Modelling cross section dependence in panel data sets is still an emerging field, but @im suggest that the appropriate test statistic is the T-bar test based on cross-sectional demeaned regressions. This is the approach that I take below (Im-Pesaran-Shin T-bar test), and the test rejects the null hypothesis at a 1% level, both when including and excluding a trend. This supports the results of @Kul [p.173]. I use the same lags as @Kul for the first IPS T-bar test, are for the full sample: Malaysia(l) and Thailand(1), for the Bretton Woods period: Thailand(1), and for the post Bretton Woods period: Malaysia(l) and Thailand(1).  

The first unit root test I employ is the Im-Pesaran-Shin t-bar test to replicate @Kul test. The results below show that the null hypothesis (there exists a unit root) is rejected at the conventional levels of significance^[I found out this is a fancy way of saying reject at 1% and 5%] \ref{ips}

```{r IPS, echo = FALSE, include= TRUE, warning =  FALSE, results='asis'}
#pg173
pdata2 <- pdata %>% select(-ENTRY)
dm <- pdata2 %>% mutate(mean = rowMeans(pdata2)) %>% mutate(rexIND = rexIND - mean, rexLNK = rexLNK - mean, rexMAL = rexMAL - mean, rexMNR = rexMNR - mean, rexPAK = rexPAK - mean, rexPHL = rexPHL - mean, rexTHI = rexTHI - mean) %>% select(-mean) #demeaned data
 
dmbret <- pdata2 %>% mutate(mean = rowMeans(pdata2)) %>% mutate(rexIND = rexIND - mean, rexLNK = rexLNK - mean, rexMAL = rexMAL - mean, rexMNR = rexMNR - mean, rexPAK = rexPAK - mean, rexPHL = rexPHL - mean, rexTHI = rexTHI - mean) %>% select(-mean) %>% slice_head(n =183) #prefloat era bretton woods demeaned data

dmpostbret<- pdata2 %>% mutate(mean = rowMeans(pdata2)) %>% mutate(rexIND = rexIND - mean, rexLNK = rexLNK - mean, rexMAL = rexMAL - mean, rexMNR = rexMNR - mean, rexPAK = rexPAK - mean, rexPHL = rexPHL - mean, rexTHI = rexTHI - mean) %>% select(-mean) %>% slice_tail(n = 195) #post bretton woods demeaned data

library(xtable)
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
 full <- ips(dm)
 bre <- ips(dmbret)
 post <- ips(dmpostbret)
 time <- data.frame(Period = c("Full Sample", "", "Bretton Woods","", "Post Bretton Woods", ""))
 data1 <- bind_rows(full, bre, post)
 data2 <- bind_cols(time, data1)
data = data2 %>% tibble::as_tibble()
  addtorow          <- list()
  addtorow$pos      <- list()
  addtorow$pos[[1]] <- c(0)
  addtorow$command  <- c(paste("\\hline \n",
                               "\\endhead \n",
                               "\\hline \n",
                               "{\\footnotesize Continued on next page} \n",
                               "\\endfoot \n",
                               "\\endlastfoot \n",sep=""))
table <- xtable(data, caption = "IPS Panel Unit Root Tests (Tbar)", label = "ips")
  print.xtable(table,
             tabular.environment = "longtable",
             floating = FALSE, # Leave this as is.
             table.placement = 'H', # Leave this as is.
             booktabs = T, # Aesthetics
             include.rownames = FALSE,  # Typically you don't want this in a table.
             add.to.row = addtorow, # For adding the Continued on next page part...
             comment = FALSE,
             caption.placement = 'top',  # Where do you want the caption?
             size="\\fontsize{12pt}{13pt}\\selectfont"  # Size of text in table..
             )

```
I then tests for unit roots using LLL test (I used the package by @plm for this).

The function panel_test performs a test on a multivariate (panel) time series by testing the null hypothesis that all series have a unit root. A rejection is typically interpreted as evidence that a ‘significant proportion’ of the series is stationary, although how large that proportion is - or which series are stationary - is not given by the test. The test is based on averaging the individual test statistics, also called the Group-Mean (GM) test in Palm, Smeekes and Urbain (2011).


```{r Boot, echo = FALSE, include= TRUE, warning =  FALSE, results='asis'}
# pdata2 <- pdata %>% select(-ENTRY)
# dm <- pdata2 %>% mutate(mean = rowMeans(pdata2)) %>% mutate(rexIND = rexIND - mean, rexLNK = rexLNK - mean, rexMAL = rexMAL - mean, rexMNR = rexMNR - mean, rexPAK = rexPAK - mean, rexPHL = rexPHL - mean, rexTHI = rexTHI - mean) %>% select(-mean)
# 
# library(xtable)
# list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
#  palm <- panel(pdata2, dm)
#  table <- xtable(palm, caption = "Bootstrapped panel unit root tests \\label{tab1}")
#   print.xtable(table,
#              # tabular.environment = "longtable",
#              floating = TRUE,
#              table.placement = 'H',
#              include.rownames = FALSE,
#              # scalebox = 0.3,
#              comment = FALSE,
#              caption.placement = 'top'
#              )

```


# Variance Ratio Test \label{Var}

The following table shows results of the Variance Ratio test for the full sample for up to 20 months. The results of the variance ratio test for the Bretton Woods period and post Bretton Woods period (for up to 20 months^[The results for 190 months is available upon request; it has been omitted purely to save space]) can be found in the Appendix (\ref{A})


```{r Vtab, results = 'asis'}

library(xtable)
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
 VR <-cvar_table(pdata)
 VRtab <- vtab(VR) %>% slice_head(n=40)
data = VRtab %>% tibble::as_tibble()
  addtorow          <- list()
  addtorow$pos      <- list()
  addtorow$pos[[1]] <- c(0)
  addtorow$command  <- c(paste("\\hline \n",
                               "\\endhead \n",
                               "\\hline \n",
                               "{\\footnotesize Continued on next page} \n",
                               "\\endfoot \n",
                               "\\endlastfoot \n",sep=""))
table <- xtable(data, caption = "Variance Ratio Test for Full Sample Up to month 20")
  print.xtable(table,
             tabular.environment = "longtable",
             floating = FALSE, # Leave this as is.
             table.placement = 'H', # Leave this as is.
             booktabs = T, # Aesthetics
             include.rownames = FALSE,  # Typically you don't want this in a table.
             add.to.row = addtorow, # For adding the Continued on next page part...
             comment = FALSE,
             caption.placement = 'top',  # Where do you want the caption?
             size="\\fontsize{12pt}{13pt}\\selectfont"  # Size of text in table..
             )
```


```{r, warning =  FALSE, fig.align = 'center', fig.cap = "Caption Here ", fig.height = 3, fig.width = 6, dev = 'png'}

# Although the functions above are really simple, the principle is simple: containing calculations and data wrangling in their own functions will make this template much cleaner and more manageable.
# When you start working, delete these meaningless functions and replace with your own...

```

# Conclusion


\newpage

# References {-}

<div id="refs"></div>

\newpage

# Appendix: \label{A} {-}

```{r Vtab2, results = 'asis'}

library(xtable)
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
 VR <-cvar_table(bret)
 VRtab <- vtab(VR)  %>% slice_head(n=40)
data = VRtab %>% tibble::as_tibble()
  addtorow          <- list()
  addtorow$pos      <- list()
  addtorow$pos[[1]] <- c(0)
  addtorow$command  <- c(paste("\\hline \n",
                               "\\endhead \n",
                               "\\hline \n",
                               "{\\footnotesize Continued on next page} \n",
                               "\\endfoot \n",
                               "\\endlastfoot \n",sep=""))
table <- xtable(data, caption = "Variance Ratio Test for Bretton Woods period up to month 20")
  print.xtable(table,
             tabular.environment = "longtable",
             floating = FALSE, # Leave this as is.
             table.placement = 'H', # Leave this as is.
             booktabs = T, # Aesthetics
             include.rownames = FALSE,  # Typically you don't want this in a table.
             add.to.row = addtorow, # For adding the Continued on next page part...
             comment = FALSE,
             caption.placement = 'top',  # Where do you want the caption?
             size="\\fontsize{12pt}{13pt}\\selectfont"  # Size of text in table..
             )
# See https://cran.r-project.org/web/packages/xtable/vignettes/xtableGallery.pdf for table inspiration
```

```{r Vtab3, results = 'asis'}

library(xtable)
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
 VR <-cvar_table(postbret)
 VRtab <- vtab(VR)  %>% slice_head(n=40)
data = VRtab %>% tibble::as_tibble()
  addtorow          <- list()
  addtorow$pos      <- list()
  addtorow$pos[[1]] <- c(0)
  addtorow$command  <- c(paste("\\hline \n",
                               "\\endhead \n",
                               "\\hline \n",
                               "{\\footnotesize Continued on next page} \n",
                               "\\endfoot \n",
                               "\\endlastfoot \n",sep=""))
table <- xtable(data, caption = "Variance Ratio Test for post Bretton Woods period up to 20 months")
  print.xtable(table,
             tabular.environment = "longtable",
             floating = FALSE, # Leave this as is.
             table.placement = 'H', # Leave this as is.
             booktabs = T, # Aesthetics
             include.rownames = FALSE,  # Typically you don't want this in a table.
             add.to.row = addtorow, # For adding the Continued on next page part...
             comment = FALSE,
             caption.placement = 'top',  # Where do you want the caption?
             size="\\fontsize{12pt}{13pt}\\selectfont"  # Size of text in table..
             )
```



